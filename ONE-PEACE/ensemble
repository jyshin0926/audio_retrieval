import torch.nn

class MyEnsemble(nn.Module):
    def __init__(self, modelA, modelB, nb_classes=2):
        super(MyEnsemble, self).__init__()
        self.modelA = modelA
        self.modelB = modelB
        
        # Remove last linear layer
        self.modelA.fc = nn.Identity()
        self.modelB.fc = nn.Identity()
        
        # Create new classifier
        self.classifier = nn.Linear( 2048+ 512, nb_classes)
        
    def forward(self, x1, x2):
        x1 = self.modelA(x1)  
        x1 = x1.view(x1.size(0), -1)
        x2 = self.modelB(x2)
        x2 = x2.view(x2.size(0), -1)
        x = torch.cat((x1, x2), dim=1)
        
        x = self.classifier(F.relu(x))
        return x

class Conv(nn.Module):
    def __init__(self):
        super(Conv, self).__init__()
        # Train your separate models
        # ...
        # We use pretrained torchvision models here
        self.modelA = models.resnet50(pretrained=True)       #model to extract common feature
        self.modelB = models.resnet18(pretrained=True)
        self.modelA_1 = copy(self.modelA)
        self.modelC = deepcopy(self.modelB)

        # Freeze these models
        for param in self.modelA.parameters():
            param.requires_grad_(False)

        for param in self.modelB.parameters():
            param.requires_grad_(False)

        for param in self.modelC.parameters():
            param.requires_grad_(False)

        self.model1 = MyEnsemble(self.modelA, self.modelB)
        self.model2 = MyEnsemble(self.modelA_1, self.modelC)

    def forward(self, d1, d2, tgt):
        # Create ensemble model
        output1 = self.model1(d1, tgt)
        output2 = self.model2(d2, tgt)
        return output1, output2


x = torch.randn(1, 3, 224, 224)
y = torch.randn(1, 3, 224, 224)
z = torch.randn(1, 3, 224, 224)
model = Conv()
out1, out2 = model(x, y, z)